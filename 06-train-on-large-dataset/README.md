# TensorFlow 2 Tutorial 6: Train on Large Dataset

This tutorial explains how to handle large datasets for training in TensorFlow 2.

If you have a large dataset (i.e., tens of GB data), they likely cannot fit your memory. In that case, it is better to design data pipeline to read data on the fly instead of saving the whole data in the memory.

The one way of the handling large dataset is using ```Model.fit_generator``` instead of ```Model.fit```. In that case, the data is created by a Generator when it is demanded during the training phase. [Original documentation](https://keras.io/models/sequential/#fit_generator) says that *the generator is run in parallel to the model, for efficiency. For instance, this allows you to do real-time data augmentation on images on CPU in parallel to training your model on GPU.* The point in here is that data augmentation on images on CPU may be optimized if the augmentation process is moved on the GPU, as well.

For efficient data reading and augmentation for large dataset, we will use ```tf.data.Dataset``` object. It allows us to apply custom mapping functions (e.g., for data preprocessing), splitting data into batches, shuffling etc. Although there are several ways of creating ```tf.data.Dataset``` object (such as from numpy arrays  using ```from_tensor_slices```), we look into creating a dataset from a generator. So we can create training or testing data on the fly and process it in GPU. 
  

## Reproduction

```

python train_on_large_dataset.py

```

  
  

## Use Checkpoint

  

First, we define the generator for training data:

```
def  train_generator():

	for image_path, label in  zip(train_images_path, train_labels):

		X = np.array(resize(imread(image_path),  (HEIGHT,  WIDTH)))

		y = label

		yield X, y

```
**return** sends a specified value back to its caller whereas **yield** can produce a sequence of values. The yield statement suspends functionâ€™s execution and sends a value back to caller, but retains enough state to enable function to resume where it is left off. [See for more info: return vs yield](https://www.geeksforgeeks.org/use-yield-keyword-instead-return-keyword-python/) 
  

The above code creates the sequences of image data and label.

Next, we create the dataset object:

```

train_dataset = tf.data.Dataset.from_generator(generator  = train_generator,
			output_types  =  (tf.float32, tf.int8),
			output_shapes=(tf.TensorShape([HEIGHT,  WIDTH,  3]), tf.TensorShape([])))
```

  
The ```from_generator``` creates ```tf.data.Dataset``` object for given sequences. While creating the object, we have to define **generator** and **output_types** arguments.  **generator** is simply the generator that we created, and **output_types** indicates the types of variables generated by the generator. **output_shapes** indicates the shapes of yielded parameters which is an image and a label in our case.

Then, we can use **train_dataset** as regular ```tf.data.Dataset```  object. For example, we can apply preprocessing etc.:


```
train_dataset = train_dataset.shuffle(NUM_TRAIN_SAMPLES).map(preprocess_train).batch(BS_PER_GPU,  drop_remainder=True)
```

## Summary

  

This tutorial explained how to hande large datasets using generators and ```tf.data.Dataset```. 

  

Use this [repo](https://github.com/lambdal/TensorFlow2-tutorial/bozcani/master/06-train-on-large-dataset) to reproduce the results in this tutorial.
